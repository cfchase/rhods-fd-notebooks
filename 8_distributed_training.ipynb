{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Fraud Detection using Codeflare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fraud detection model is very small and quickly trained.  However, for many large models, training requires multiple GPUs and often multiple machines.  In this notebook, we'll demonstrate how to train a model using Ray on OpenShift AI to scale out the training.  We'll use the Codeflare SDK to create the cluster and submit the job.  Full documentation for the SDK is available [here](https://project-codeflare.github.io/codeflare-sdk/detailed-documentation/)\n",
    "\n",
    "For this demo we require codeflare-sdk of at least 0.19.1.  Let's begin by installing the SDK if it's not already installed or up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade codeflare-sdk>=0.19.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "Normally data would already be in a shared location, but since it's local, we'll upload it to our object storage so we can show sharded data loading from a shared data source. Once it's uploaded, we can work with it using Ray Data so it's properly sharded across the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./utils')\n",
    "\n",
    "import utils.s3\n",
    "\n",
    "utils.s3.upload_directory_to_s3(\"data\", \"data\")\n",
    "print(\"---\")\n",
    "utils.s3.list_objects(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate to the cluster using the OpenShift console login\n",
    "\n",
    "We'll be creating the Kubernetes objects for Ray Clusters using the Codeflare SDK.  In order to do so, you'll need permission to do so in your own namespace.  The easiest way to do this is using the `oc` client.  You can copy the oc login command from the OpenShift console.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/copy-login.png\"  alt=\"copy login\"  >\n",
    "<figure/>\n",
    "\n",
    "As a helper, you can run the below cell and click the link to take you directly to the token request page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To launch a Ray cluster, you will need to authenticate yourself against the OpenShift cluster.\n",
    "# Run this cell to get the full instructions.\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "NOTEBOOK_ARGS = os.environ.get('NOTEBOOK_ARGS', '')\n",
    "match = re.search(r'\"hub_host\":\"https://.*?(apps\\.[^\"]+)\"', NOTEBOOK_ARGS)\n",
    "hub_host_value = match.group(1)\n",
    "\n",
    "login_url = 'https://oauth-openshift.' + hub_host_value + \"/oauth/token/request\"\n",
    "\n",
    "print('Open the following URL to get your authentication token.')\n",
    "print('Authenticate, then click on \"Display token\", and copy the content of the line under \"Log in with this token\"')\n",
    "print('You can then come back here and paste this content in the next cell.')\n",
    "print('Login URL: ' + login_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the `oc login` command the same way we run any terminal command.  You can designate a cell as `%%bash` or you can paste a command after `!`.\n",
    "\n",
    "Paste the `oc login --token=sha256~XXXX --server=https://XXXX` command in the following cell designated with `%%bash` and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc login --token=sha256~XXXX --server=https://XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a Ray Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure our Ray cluster\n",
    "\n",
    "CodeFlare allows you to specify many parameters such as number of workers, image, and kueue local queue name.  A full list is available [here](https://project-codeflare.github.io/codeflare-sdk/detailed-documentation/cluster/config.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from codeflare_sdk import Cluster, ClusterConfiguration\n",
    "\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name=\"raycluster-cpu\",\n",
    "    head_gpus=0,\n",
    "    num_gpus=0,\n",
    "    num_workers=2,\n",
    "    min_cpus=1,\n",
    "    max_cpus=4,\n",
    "    min_memory=2,\n",
    "    max_memory=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the cluster\n",
    "\n",
    "This will create the necessary Kubernetes objects to run the Ray cluster.  It may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.up()\n",
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, get a running cluster object\n",
    "\n",
    "This is useful when you've already created a cluster, but you've restarted the Python kernel, closed the notebook, or are working in a different notebook.\n",
    "\n",
    "Uncomment below to connect to an existing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from codeflare_sdk import get_cluster\n",
    "\n",
    "# cluster = get_cluster(name, namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can show information about the cluster, including a link to the dashboard.  There we can inspect the running jobs and logs, and see the resources being used.\n",
    "<figure>\n",
    "    <img src=\"./assets/codeflare-details.png\"  alt=\"codeflare details\" width=\"400\">\n",
    "<figure/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link to the Ray dashboard is available in the details above.  It should look something like this:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"./assets/ray-dashboard.png\"  alt=\"ray dashboard\" width=\"600\"\n",
    "<figure/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Job Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Job Submission Client\n",
    "\n",
    "We want to connect to the running Ray Cluster to submit jobs.  We can do this by initializing the job client which will have the proper authentication and connection information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = cluster.job_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're connected, we can query the cluster.  Let'ssSee if there are any existing jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.list_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a Runtime Environment\n",
    "\n",
    "Now we can set up the [runtime environment](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments) for the job.  This includes the working directory, files to exclude, dependencies, and environment variables.\n",
    "\n",
    "```python\n",
    "runtime_env={\n",
    "    \"working_dir\": \"./\", # relative path to files uploaded to the job\n",
    "    \"excludes\": [\"local_data/\"], # directories and files to exclude from being uploaded to the job\n",
    "    \"pip\": [\"boto3\", \"botocore\"], # can also be a string path to a requirements.txt file\n",
    "    \"env_vars\": {\n",
    "        \"MY_ENV_VAR\": \"MY_ENV_VAR_VALUE\",\n",
    "        \"MY_ENV_VAR_2\": os.environ.get(\"MY_ENV_VAR_2\"),\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# script = \"test_data_loader.py\"\n",
    "script = \"train_tf_cpu.py\"\n",
    "runtime_env = {\n",
    "    \"working_dir\": \"./ray-scripts\",\n",
    "    \"excludes\": [],\n",
    "    \"pip\": \"./ray-scripts/requirements.txt\",\n",
    "    \"env_vars\": {\n",
    "        \"AWS_ACCESS_KEY_ID\": os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n",
    "        \"AWS_SECRET_ACCESS_KEY\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        \"AWS_S3_ENDPOINT\": os.environ.get(\"AWS_S3_ENDPOINT\"),\n",
    "        \"AWS_DEFAULT_REGION\": os.environ.get(\"AWS_DEFAULT_REGION\"),\n",
    "        \"AWS_S3_BUCKET\": os.environ.get(\"AWS_S3_BUCKET\"),\n",
    "        \"NUM_WORKERS\": \"1\",\n",
    "        \"TRAIN_DATA\": \"data/train.csv\",\n",
    "        \"VALIDATE_DATA\": \"data/validate.csv\",\n",
    "        \"MODEL_OUTPUT_PREFIX\": \"models/fraud/1/\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Submit the configured job\n",
    "\n",
    "Now we can submit the job to the cluster.  This will create the necessary Kubernetes objects to run the job.  The job will run the script with the specified runtime environment.  The script is located in [ray-scripts/train_tf_cpu.py](./ray-scripts/train_tf_cpu.py).  The script follows the code fairly closely to the official [Ray TensorFlow example](https://docs.ray.io/en/latest/train/distributed-tensorflow-keras.html).  While we're using TensorFlow, there are examples for PyTorch and other frameworks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission_id = client.submit_job(\n",
    "    entrypoint=f\"python {script}\",\n",
    "    runtime_env=runtime_env,\n",
    ")\n",
    "\n",
    "print(submission_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query Important Job Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the job's status\n",
    "print(client.get_job_status(submission_id), \"\\n\")\n",
    "\n",
    "# Get job related info\n",
    "print(client.get_job_info(submission_id), \"\\n\")\n",
    "\n",
    "# Get the job's logs\n",
    "print(client.get_job_logs(submission_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also tail the job logs to watch the progress of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through the logs of a job \n",
    "async for lines in client.tail_job_logs(submission_id):\n",
    "    print(lines, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.list_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stop jobs\n",
    "\n",
    "If we want to stop a job, we can do so by calling `stop_job` with the submission id.  Here we list all the jobs and stop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T21:46:51.437556Z",
     "start_time": "2024-08-28T21:46:51.321167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for job_details in client.list_jobs():\n",
    "    print(f\"deleting {job_details.submission_id}\")\n",
    "    client.stop_job(job_details.submission_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Delete jobs\n",
    "\n",
    "We can also delete the jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for job_details in client.list_jobs():\n",
    "    print(f\"deleting {job_details.submission_id}\")\n",
    "    client.delete_job(job_details.submission_id)\n",
    "\n",
    "client.list_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the \n",
    "\n",
    "Once we're done training, we can delete the cluster.  This will remove the Kubernetes objects and free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
