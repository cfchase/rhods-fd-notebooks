[id='distributed-jobs-with-ray']
= Distributing training jobs with Ray

We've trained this model several ways now, directly in a notebook, in a pipeline, and now we'll train it using Ray. Ray is a distributed computing framework that can be used to parallelize Python code across multiple CPUs or GPUs. In this section, you'll learn how to use Ray to distribute the training of a machine learning model across multiple CPUs.  While this is not necessary for this simple model, it's a good way to learn how to use Ray for more complex models that require more compute power, often multiple GPUs across multiple machines.

In your notebook environment, open the `8_distributed_training.ipynb` file and follow the instructions directly in the notebook. The instructions guide you through some authentication, creating Ray clusters, and working with jobs.

If you'd like to take a closer look at the python code that you'll be running, you can find it in the ray-scripts directory `ray-scripts/train_tf_cpu.py`.  For more information about TensorFlow training on Ray, chack out the https://docs.ray.io/en/latest/train/distributed-tensorflow-keras.html[Ray TensorFlow guide]

image::distributed/jupyter-notebook.png[Jupyter Notebook]

.Next step

xref:preparing-a-model-for-deployment.adoc[Preparing a model for deployment]
